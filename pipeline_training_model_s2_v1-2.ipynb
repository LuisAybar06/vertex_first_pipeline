{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c97b043-edcd-46ab-93f5-7f159ca10cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el CSV\n",
    "df = pd.read_csv('data/y_train/ytrain.csv')\n",
    "\n",
    "df['SalePrice'] = df['SalePrice'].apply(lambda x: str(x).replace('.', ''))\n",
    "\n",
    "df['SalePrice'] = df['SalePrice'].astype(int)\n",
    "\n",
    "df.to_csv('data/pipeline_train_model_v1-1/ytrain.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d632b831-1c9f-4105-b633-b5de49b7a7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job 4b8c31dd-0927-4458-8496-952dfbc583ff for xtrain.csv\n",
      "Job 4b8c31dd-0927-4458-8496-952dfbc583ff finished for xtrain.csv\n",
      "Loaded 1314 rows into int-advanced-analytics-01.expl_research_laybarm.xtrain.\n",
      "Starting job b70a4d54-c1c0-4d0a-8f63-fcdb729ff9bb for ytrain.csv\n",
      "Job b70a4d54-c1c0-4d0a-8f63-fcdb729ff9bb finished for ytrain.csv\n",
      "Loaded 1314 rows into int-advanced-analytics-01.expl_research_laybarm.ytrain.\n",
      "Starting job 02cbc373-836b-49b3-a16a-bebc68df7bc1 for selected_features.csv\n",
      "Job 02cbc373-836b-49b3-a16a-bebc68df7bc1 finished for selected_features.csv\n",
      "Loaded 36 rows into int-advanced-analytics-01.expl_research_laybarm.selected_features.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "# Cliente BigQuery\n",
    "client = bigquery.Client()\n",
    "\n",
    "project_id = \"<project_id>\"\n",
    "dataset_id = \"<dataset>\"\n",
    "folder_path = \"data/pipeline_train_model_v1-1\"\n",
    "\n",
    "# Configurar la tabla de destino\n",
    "dataset_ref = client.dataset(dataset_id)\n",
    "\n",
    "# Iterar sobre los archivos en la carpeta\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        table_id = os.path.splitext(file_name)[0] \n",
    "        table_ref = dataset_ref.table(table_id)\n",
    "\n",
    "        # Configurar el job de carga\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.CSV,\n",
    "            skip_leading_rows=1,\n",
    "            autodetect=True,\n",
    "        )\n",
    "\n",
    "        csv_file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Leer el archivo CSV y cargarlo a BigQuery\n",
    "        with open(csv_file_path, \"rb\") as source_file:\n",
    "            load_job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "\n",
    "        print(f\"Starting job {load_job.job_id} for {file_name}\")\n",
    "        load_job.result()  # Esperar a que termine el trabajo\n",
    "        print(f\"Job {load_job.job_id} finished for {file_name}\")\n",
    "\n",
    "        destination_table = client.get_table(table_ref)\n",
    "        print(f\"Loaded {destination_table.num_rows} rows into {table_ref}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c7bdd71-c11a-49a6-b372-9ee5613f6a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import dsl, compiler\n",
    "from kfp.v2.dsl import (Artifact, ClassificationMetrics, Input, Metrics, Output, component,Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "44e57077-1655-4f4e-9356-4f506944b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"google-cloud-bigquery-storage\",\n",
    "        \"pandas\",\n",
    "        \"db-dtypes\",\n",
    "        \"pyarrow\"\n",
    "    ],\n",
    ")\n",
    "def process_data(\n",
    "    project: str,\n",
    "    source_x_train_table: str,\n",
    "    features_table: str,\n",
    "    dataset: Output[Dataset],\n",
    "):\n",
    "    \n",
    "    import sys\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import pyarrow.parquet as pq\n",
    "\n",
    "    client = bigquery.Client(project=project)\n",
    "    \n",
    "    \n",
    "    X_train = client.query(\n",
    "    '''SELECT * FROM `{dsource_table}`\n",
    "        '''.format(dsource_table=source_x_train_table)).to_dataframe()\n",
    "    \n",
    "    features = client.query(\n",
    "    '''SELECT * FROM `{dsource_table}`\n",
    "        '''.format(dsource_table=features_table))\n",
    "    \n",
    "    df = features.to_dataframe()\n",
    "    \n",
    "    # Seleccionar caracter√≠sticas\n",
    "    features = df[\"string_field_0\"].tolist()\n",
    "\n",
    "    X_train = X_train[features]\n",
    "\n",
    "    X_train.to_parquet(f'{dataset.path}.parquet',engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a987ad19-6a8f-4c8f-a0fa-12568b03ca58",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"google-cloud-bigquery-storage\",\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"joblib\",\n",
    "        \"db-dtypes\"\n",
    "    ],\n",
    ")\n",
    "def train_model(\n",
    "    project: str,\n",
    "    source_y_train_table: str,\n",
    "    inputd: Input[Dataset],\n",
    "):\n",
    "    \n",
    "    import sys\n",
    "    from google.cloud import bigquery\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import Lasso\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "\n",
    "    client = bigquery.Client(project=project)\n",
    "    \n",
    "\n",
    "    Y_train = client.query(\n",
    "    '''SELECT * FROM `{dsource_table}`\n",
    "        '''.format(dsource_table=source_y_train_table)).to_dataframe()\n",
    "\n",
    "    \n",
    "    # Leer datos de Parquet del conjunto de datos de entrada   \n",
    "    X_train = pd.read_parquet(f'{inputd.path}.parquet')\n",
    "    \n",
    "    \n",
    "    # Configurar el modelo\n",
    "    lin_model = Lasso(alpha=0.001, random_state=0)\n",
    "    \n",
    "    # Train model \n",
    "    lin_model.fit(X_train, Y_train)\n",
    "    \n",
    "    model_filename = 'lasso_model.joblib'\n",
    "    joblib.dump(lin_model, model_filename)\n",
    "    \n",
    "    \n",
    "    # Upload the model to GCS\n",
    "    storage_client = storage.Client(project=project)\n",
    "    bucket_name = \"<bucket>\"\n",
    "    destination_blob_name = \"demo/data/model/model.joblib\"\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "22d6605f-8ca6-4287-bc46-7ad62a8cabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name=\"pipeline-training-model\", \n",
    "    description=\"intro\",\n",
    "    pipeline_root=\"gs://<bucket>/demo\"\n",
    ")\n",
    "def main_pipeline(\n",
    "    project: str,\n",
    "    source_x_train_table: str,\n",
    "    source_y_train_table: str,\n",
    "    features_table: str,\n",
    "    gcp_region: str = \"us-central1\",\n",
    "):\n",
    "    get_data = process_data(\n",
    "        project = project,\n",
    "        source_x_train_table = source_x_train_table,\n",
    "        features_table = features_table\n",
    "    )\n",
    "    get_data.set_display_name(\"PROCESS_DATA\")\n",
    "    \n",
    "    train = train_model(\n",
    "        project = project,\n",
    "        source_y_train_table = source_y_train_table,\n",
    "        inputd = get_data.output\n",
    "    ).after(get_data)\n",
    "    train.set_display_name(\"TRAIN_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9a09581f-2348-45b5-8533-225e3d04d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=main_pipeline,\n",
    "    package_path=\"pipeline_training.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d30531d7-f798-420e-9cde-d8bd781deb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=\"<project_id>\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "717e7da3-41a5-4a91-b763-a36acfdc6c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit pipeline job ...\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/624205664083/locations/us-central1/pipelineJobs/pipeline-training-model-20240624041020\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/624205664083/locations/us-central1/pipelineJobs/pipeline-training-model-20240624041020')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/pipeline-training-model-20240624041020?project=624205664083\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"pipeline de prueba\",\n",
    "    template_path=\"pipeline_training.json\",\n",
    "    #job_id=\"mlops\",\n",
    "    enable_caching=False,\n",
    "    project=\"<project_id>\",\n",
    "    location=\"us-central1\",\n",
    "    parameter_values={\"project\": \"<project_id>\", \n",
    "                      \"source_x_train_table\": \"<project_id>.<dataset>.xtrain\",\n",
    "                      \"source_y_train_table\": \"<project_id>.<dataset>.ytrain\",\n",
    "                      \"features_table\": \"<project_id>.<dataset>.selected_features\"\n",
    "                     }\n",
    "    #labels={\"module\": \"ml\", \"application\": \"app\", \"chapter\": \"mlops\", \"company\": \"datapat\", \"environment\": \"dev\", \"owner\": \"xxxx\"}\n",
    ")\n",
    "\n",
    "print('submit pipeline job ...')\n",
    "job.submit(service_account=\"dev-dp-ml-vertex@<project_id>.iam.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5527304a-3e45-4a07-a559-da4ebfe6abe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
